import streamlit as st
import pandas as pd
pd.set_option('display.max_columns', 60)
import numpy as np


### INT√âGRATION DU FICHIER CSS 
with open('style.css') as f:
    css = f.read()

st.markdown(f'<style>{css}</style>', unsafe_allow_html=True)

#### TITRE DU STREAMLIT
st.title("Temps de r√©ponse - Brigade des Sapeurs Pompiers de Londres")

#### CR√âATION DE LA SIDEBAR
sidebar_title = '<p style="color:White; font-size: 26px;">Sommaire</p>'
st.sidebar.markdown(sidebar_title, unsafe_allow_html=True)

pages=["Introduction", "Enrichissements & Data Cleaning","DataVizualization", "Mod√©lisation",
       "Conclusion"]
page=st.sidebar.radio('', pages)

st.sidebar.divider()
st.sidebar.markdown('Auteurs')

st.sidebar.markdown('- <p style="font-size: 14px"> Alia Boudehane | <a href="https://www.linkedin.com/in/alia-boudehane-704172185/" style="color: white;font-size: 14px">LinkedIn</a></p>\
                    \n- <p style="font-size: 14px"> Doravann Chou | <a href="https://www.linkedin.com/in/doravann-chou-81a999269" style="color: white;font-size: 14px">LinkedIn</a></p>\
                    \n- <p style="font-size: 14px"> Ma√Øna Le Roux | <a href="https://www.linkedin.com/in/mainaleroux" style="color: white;font-size: 14px">LinkedIn</a> </p>', unsafe_allow_html=True)


#### PAGE 1 : INTRODUCTION

if page == pages[0] : 
  st.image('lfb2.jpeg')
  st.header("Introduction")
  st.subheader('Le Sujet')
  st.markdown("La **:red[London Fire Brigade (LFB)]** est le service d'incendie et de secours le plus actif du pays.\
           Fond√©e en 1886, elle n'a, depuis lors, cess√© de s‚Äôam√©liorer et jouit aujourd‚Äôhui d‚Äôune r√©putation reconnue\
            en mati√®re de lutte contre les incendies et de secours d'urgence, faisant d‚Äôelle l'une des institutions les\
            plus embl√©matiques et respect√©es du pays. \n\n Sur l‚Äôann√©e 2014, ils seront notifi√©s comme √©tant les sapeurs-pompiers les plus occup√©s\
            dans le monde en comptabilisant un total de 171 067 appels d‚Äôurgence et en traitant 20934 feux ! \
           \nC‚Äôest d‚Äôailleurs l‚Äôune des plus grandes organisations de lutte contre les incendies et de secours au monde, agissant dans la protection des personnes\
            et biens contre les incendies sur un p√©rim√®tre de 1587 kilom√®tres carr√©s autour du Grand Londres.\
           \n\n Dans cette √©tude, nous nous plongeons dans l'analyse des donn√©es li√©es aux performances pass√©es de la London Fire Brigade, en mettant l'accent\
            sur leur **:red[efficacit√© de temps de r√©ponse].** \n\nNous avons dans un premier temps prendre connaissance de notre jeu de donn√©es\
            qui provient du site officiel et gouvernemental de la London Fire Brigade. Nous avons √©tudi√© puis nettoy√© ces donn√©es afin de les rendre\
            les plus lisibles et utiles √† notre √©tude.\
           \n\n Nous avons utilis√© ensuite des techniques d'analyse de donn√©es et des mod√®les de pr√©diction pour mieux comprendre les tendances historiques\
            et les facteurs qui influencent ces mesures cruciales.")


  st.subheader('Les Donn√©es')
  st.markdown('**:red[Source des Jeux de Donn√©es]**')
  st.markdown("Nos jeux donn√©es sont disponibles sur le site officiel de la ville de Londres - https://data.london.gov.uk, ce qui nous permet d'avoir une enti√®re confiance\
               en leur provenance et leur int√©grit√©. \n\nDurant la phase exploratoire de notre projet, nous avons √©galement √©t√© en contact avec des Business\
               Intelligence Analysts de la London Fire Brigade, qui ont r√©pondu rapidement √† nos questions, et nous les en remercions.")
  st.markdown('**:red[Description des Donn√©es]**')
  st.markdown("Nous avions deux principaux jeux de donn√©es disponibles :\
              \n - **Incident Records** : les d√©tails de chaque incident sur lequel la LFB est intervenu depuis le 1er janvier 2009. Des informations sont\
               fournies sur la date et le lieu de l'incident ainsi que sur le type d'incident. \
              \n - **Mobilisation Records** : les d√©tails de chaque v√©hicule d'incendie envoy√© sur les lieux d'un incident depuis janvier 2009.\
              Des informations sont fournies sur l'engin mobilis√©, le lieu d'o√π il a √©t√© d√©ploy√© et les heures d'arriv√©e sur les lieux de l'incident.")
  
  col1, col2 = st.columns(2)

  with col1:
    st.write("***Incident Records***")
    metadata = pd.read_csv("Metadata.csv")
    st.dataframe(metadata)
  with col2:
    st.write("***Mobilisation Records***")
    metadata_mobi = pd.read_csv("Mobilisations-Metadata.csv")
    st.dataframe(metadata_mobi)

  st.markdown("Nous disposons de 3 colonnes communes aux 2 jeux de donn√©es (*IncidentNumber*, *Calyear*, et *HourOfCall*) √† partir desquelles\
               nous allons **fusionner nos tables**.")
  
  st.markdown('**:red[DataFrame Consolid√©]**')
  st.markdown("Voici un aper√ßu de notre DataFrame consolid√©, avant toute modification :")

  ## Afficher df.head() apr√®s fusion
  df_consolidated = pd.read_csv("df_consolidated.csv",index_col = 0)
  df_consolidated.reset_index(inplace = True)
  df_consolidated = df_consolidated.drop(columns = 'index')
  st.dataframe(df_consolidated)

  st.write("La taille de notre dataframe est : `(2220718, 58)`")

  st.markdown("Pour la suite du projet et notamment la partie mod√©lisation, c'est la variable ***:red[AttendanceTimeSeconds]*** que nous choisissons comme\
              notre **variable cible**. Il s'agit du temps de r√©ponse pour un camion mobilis√©, comprenant le temps de pr√©paration de la brigade\
               une fois cette derni√®re inform√©e ainsi que le temps de trajet pour ce rendre sur le lieu de l'incident.")

#### PAGE 2 : ENRICHISSEMENTS ET DATA CLEANING
if page == pages[1] : 
  st.header("Enrichissements & Data Cleaning")
  st.markdown(" ")

  st.subheader('Nos Enrichissements :fire:')
  st.markdown('**:red[La variable Distance]**')
  st.markdown("Nous avons souhait√© ajouter √† notre dataset la variable ***:red[Distance]***. Cette derni√®re repr√©sente la distance entre la caserne d'o√π a √©t√© mobilis√©\
              le camion et le lieu de l'incident, en m√®tres. Voici comment nous avons proc√©d√© : \n\n - **Collecte de donn√©es** : pour chaque caserne, nous avons manuellement\
              collect√© leurs coordonn√©es g√©ographiques, soit les latitudes et longitudes.\
              \n - **Conversion de variables** : les donn√©es de latitudes et longitudes des lieux d'incidents √©tant incompl√®tes (+50% de Nan), nous avons convertis\
               les donn√©es *easting_rounded* et *northing_rounded* en latitude et longitude. Nous perdons l√©g√®rement en pr√©cision sur la localisation exacte mais de fa√ßon\
              tout √† fait acceptable. \
              \n - **Calcul de la distance** : √† partir des coordonn√©es g√©ographiques des casernes et des lieux d'incidents, nous avons p√ª calculer une nouvelle variable : la\
               :red[***Distance*** *(en m√®tre)*] parcourue pour chaque mobilisation.\
              \n - **V√©rification** : pour se rassurer sur la pertinence du calcul effectu√©, nous avons test√© plusieurs distances sur Google Maps.")
  
  with st.expander("Calcul de la Distance (code)"):
    code = '''# Fonction pour calculer la distance en m√®tres entre deux points g√©ographiques (haversine formula)
def haversine(lat1, lon1, lat2, lon2):
    # Rayon de la Terre en m√®tres
    radius = 6371000

    # Conversion des degr√©s en radians
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])

    # Diff√©rences de latitudes et de longitudes
    dlat = lat2 - lat1
    dlon = lon2 - lon1

    # Formule de la haversine
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))

    # Distance en m√®tres
    distance = radius * c
    return distance

# Appliquer la fonction haversine pour calculer la distance et ajouter une colonne "Distance" au DataFrame
df["Distance"] = df.apply(lambda row: haversine(row["Latitude"], row["Longitude"], row["Station_Latitude"], row["Station_Longitude"]), axis=1) '''
    st.code(code,language='python')

  st.markdown(" ")
  st.markdown('**:red[La variable DateOfCall]**')
  st.markdown("Afin de rendre l'analyse plus facile et d'exploiter d'autres axes nous utilisons la variable *'DateOfCall'* pour cr√©er *'MonthOfCall'* et *'DayOfCall'*.")
  st.markdown(" ")

  st.subheader("DataCleaning :male-firefighter:")
  st.markdown("Sans rentrer dans le d√©tail, expliquer les modifications apport√©es / s√©lection des variables.\
              \n Afficher un aper√ßu de df_final\
              et aussi df.info(), autre information int√©ressante √† ce stade")
  
  st.markdown('**:red[Gestion des doublons]**')
  st.markdown("Nous n'avions aucun doublon dans notre jeu de donn√©es.")

  st.markdown('**:red[Gestion des outliers]**')
  st.markdown("Nos outliers √©taient majoritairement des valeurs extr√™mes r√©alistes. Pour les quelques valeurs aberrantes (quelques lignes), nous\
              avons fait le choix de les supprimer.")

  st.markdown('**:red[Gestion des valeurs manquantes]**')
  st.markdown("Concernant les Nans, nous d√©cidons de supprimer toutes les lignes pour lesquels le pourcentage de Nan dans la colonne est inf√©rieur √† 2%.\
               Ensuite, pour le reste des valeurs manquantes et afin d‚Äô√©viter toute fuite de donn√©es durant la mod√©lisation nous ne modifierons que les Nans\
              qui ne n√©cessitent pas une op√©ration de calcul statistique.\
              \n - *DelayCode_Description (75% de Nans)* : il s'agit de la raison pour laquelle la brigade est arriv√©e en retard sur le lieu de l‚Äôincident. Les Nans\
               correspondent aux mobilisations o√π il n'y a pas eu de retard (onfirm√© par le BI Analyst que nous avons contact√©). Nous rempla√ßons donc remplac√©s par\
                la veleur 'No Delay'.\
               \n - *SpecialServiceType (78% de Nans)* : cette colonne ne contient des informations que si le type de l‚Äôintervention est un 'SpecialService'. Nous\
               rempla√ßon les 78% de Nan par la valeur 'Not a Special Service'.\
              \n\n Suite √† la s√©lection finale des variables que nous conserverons avant d'ent√¢mer la partie mod√©lisation, notre jeu de donn√©es ne pr√©sente plus aucun Nan.")

  st.markdown('**:red[Variables Conserv√©es dans notre DataFrame Final]**')
  st.markdown("Voici la liste des variables conserv√©es (**25 au total**), avant de d√©marrer nos premi√®res mod√©lisations :\
              \n\n *'IncidentNumber', 'DateOfCall', 'CalYear', 'HourOfCall',\
              'IncidentGroup', 'StopCodeDescription', 'SpecialServiceType', 'PropertyCategory', 'AddressQualifier', 'BoroughName', 'IncidentStationGround',\
              'NumStationsWithPumpsAttending', 'NumPumpsAttending', 'CallCount', 'ResourceMobilisationId', 'TurnoutTimeSeconds', 'TravelTimeSeconds', 'AttendanceTimeSeconds',\
              'DeployedFromStation_Name', 'PumpOrder', 'DelayCode_Description', 'MobilisationTime', 'MonthOfCall', 'DayOfCall', 'Distance'*")
  
  ## Afficher df.head() avant mod√©lisation
  st.markdown('**Aper√ßu du DataFrame avant la mod√©lisation**')
  df_modelisation = pd.read_csv("df_modelisation.csv",index_col = 0)
  st.dataframe(df_modelisation)

#### PAGE 3 : DATAVIZ

if page == pages[2] : 
  st.header("DataVizualization")
  st.markdown(" ")
  st.subheader("La variable cible : AttendanceTimeSeconds")
  
  ## DataViz : distribution variable cible
  st.markdown('**:red[Distribution de AttendanceTimeSeconds]**')
  st.markdown("Nous constatons visuellement que la distribution de notre variable cible suit une loi normale. La m√©diane est √† 325 secondes,\
               soit **5 minutes et 25 secondes**. Il s'agit donc ici du temps m√©dian pour une √©quipe de sapeurs-pompiers de se pr√©parer et ce rendre\
              sur les lieux de l'incident. Le maximum est √† 1200 secondes, soit 20 min, et il s'agit du seuil maximum utilis√© par la LFB elle-m√™me\
              dans ses diff√©rents analyses et rapports.")
  st.image('attendancetime_distribution.jpeg')

  ## DataViz Attendance Time selon carte de Londres
  st.markdown(" ")
  st.markdown('**:red[Cartes de Londres]**')
  
  display = st.radio(':gray[Que souhaitez-vous montrer ?]', (':gray[Temps de R√©ponse par Quartier]', ':gray[Temps de R√©ponse par Secteur Caserne]'))
  if display == ':gray[Temps de R√©ponse par Quartier]':
    ## Map par Quartier
    path_to_html = "map2.html" 
    # Read file and keep in variable
    with open(path_to_html,'r') as f: 
        html_data = f.read()
    # Show in streamlit
    st.components.v1.html(html_data,width=800, height=600)
  elif display == ':gray[Temps de R√©ponse par Secteur Caserne]':
    ## Map par Station Ground
    path_to_html = "map1.html" 
    # Read file and keep in variable
    with open(path_to_html,'r') as f: 
        html_data = f.read()
    # Show in streamlit
    st.components.v1.html(html_data,width=800, height=600)
  
  ## DataViz Attendance Time selon heure de la journ√©e
  st.markdown(" ")
  st.markdown("**:red[Les Temps de R√©ponse selon l'Heure de la Journ√©e]**")
  st.markdown("Le graphique ci-dessous nous permet de visualiser le **Temps de Pr√©paration** (TurnoutTime), le **Temps de Trajet** (TravelTime)\
              et le **Temps de R√©ponse** total (AttendanceTime) des brigades selon l'heure de la journ√©e.")
  path_to_plot1 = "plot1.html" 
  with open(path_to_plot1,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=1000, height=450)

  with st.expander(label = "Lecture du graphique"):
    st.write("**Constat temps de trajet :** Assez logiquement, nous observons que le temps de trajet pour se rendre sur le lieu de l'incident\
              est plus important en journ√©e, au moment o√π le trafic routier est plus dense. Le temps de trajet est plus rapide entre 21h le soir et 9h le matin.\
            \n\n **Constat temps de trajet temps de pr√©paration (Turnout) :**  Il est int√©ressant de voir qu'il existe une diff√©rence du temps de pr√©paration\
             des √©quipes entre minuit et 7h du matin. Est-ce d√ª au fait qu'il y a moins de personnel la nuit ?\
             \n\n**Constat temps de r√©ponse total :** Le temps de r√©ponse total est la somme des deux temps pr√©c√©dents. Les temps forts / faibles de chacune\
              des deux variables pr√©c√©dentes font que le temps total s'√©quilibre sur la journ√©e, √† l'exception du cr√©neau 7h - 10h et  20h et Minuit (temps de trajet ET temps de pr√©paration rapides).")

  ## DataViz Temps de trajet selon le retard
  st.markdown(" ")
  st.markdown("**:red[Temps de Trajet moyen selon la raison du retard]**")
  st.markdown("Le graphique ci-dessous nous permet de visualiser le **Temps de Trajet moyen** (en secondes toujours) selon la raison du retard. Lorsqu'il n'y a pas eu de retard,\
              cela est repr√©sent√© par la valeur '*No delay*'.")
  path_to_plot5 = "plot5.html" 
  with open(path_to_plot5,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=800, height=450)

  with st.expander(label = "Lecture du graphique"):
    st.write("**Constat :** Sans surprise, lorsqu'il n'y a pas de retard le temps de trajet est exemplaire. Par ailleurs, le motif d' *'Adresse incompl√®te'*\
            est celui qui fait le plus perdre de temps aux equipes, suivi d'une autre cause moins explicite qui indique que l'√©quipe est arriv√©e mais\
            qu'elle a √©t√© retenue pour une autre raison ou encore que l'√©quipe √©tait d√©j√† en intervention √† l'exterieur au moment de l'appel par le centre de contr√¥le.")

## DataViz Temps de r√©ponse et Distance
  st.markdown(" ")
  st.markdown("**:red[Temps de Trajet moyen et Distance moyenne parcourue par Quartier]**")
  st.markdown("Le graphique ci-dessous affiche par quartier de Londres le Temps de R√©ponse moyen ainsi que la Distance moyenne parcourue")
  
  path_to_plot6 = "plot6.html" 
  with open(path_to_plot6,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=1000, height=450)

  with st.expander(label = "Lecture du graphique"):
    st.write("**Constat :** Alors que nous pensions plus √©vidente la corr√©lation entre le Temps de R√©ponse et la Distance, nous constatons ici, en regardant ces deux m√©triques\
             par Quartier de Londres, qu'il n'existe pas de fa√ßon √©vidente une relation entre ces deux variables.")


  st.markdown(" ")
  st.subheader("Le volume d'Incidents et de Mobilisations")

  ## DataViz Nombre d'incidents par ann√©e
  st.markdown("**:red[Volume d'incidents par Ann√©e]**")
  st.markdown("Le graphique ci-dessous nous permet de visualiser le **Volume d'incidents** selon l'ann√©e, avec une disctinction faite selon le **Type d'incident**.")
  path_to_plot7 = "plot7.html" 
  with open(path_to_plot7,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=900, height=450)

  with st.expander(label = "Lecture du graphique"):
    st.write("**Constat :** Nous constatons une grande proportion de fausse alarme dans laquelle nous avons une majorit√© AFA- Automatic Fire Alarm (alarme d√©clench√©e automatiquement\
            par les detecteurs de fum√©e). Concernant les feux il y a majoritairement des feux de grande ampleur.\
             \n\n Au cours des ann√©es le volume d'incidents varie sensiblement, cependant nous constatons un accroissement constant depuis 2015.\
             \n\n L'ann√©e 2023 n'√©tant pas finie elle n'est pas encore √©ligible √† une bonne lecture, nos donn√©es s'arretent √† Juillet 2023 soit 60% de l'ann√©e , si la tendance moyenne\
              d'√©volution du volume d'incident du mois reste la m√™me nous pourrions nous attendre √† avoir un volume d√©passant les 170 000 incidents d'ici la fin de l'ann√©e.\
             Une estimation qui vient se rapprocher des r√©sultats de l'ann√©e 2022.") 

  ## DataViz Nombre d'incident selon heure de la journ√©e
  st.markdown(" ")
  st.markdown("**:red[Nombre d'Incidents selon Heure de la Journ√©e]**")
  st.markdown("Le graphique ci-dessous nous permet de visualiser le **Volume d'incidents** selon l'heure de la journ√©e, avec une disctinction faite selon le **Type d'incident**.\
              Il est √©galement possible d'afficher l'ann√©e de son choix.")
  path_to_plot2 = "plot2.html" 
  with open(path_to_plot2,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=900, height=450)

  with st.expander(label = "Lecture du graphique"):
    st.write("**Constat :** Il est nettement visible - et c'est assez logique - qu'il existe une diff√©rence du volume d‚Äôincidents selon l‚Äôheure de la journ√©e\
            et cela quelque soit l'ann√©e. Le graphique nous montre √©galement une surrepr√©sentation des incidents de type ‚ÄúFalse Alarm‚Äù.")

  ## DataViz Nombre de mobilisation par quartier
  st.markdown(" ")
  st.markdown("**:red[R√©partition des Mobilisations par Quartier]**")
  st.markdown("Le graphique ci-dessous nous permet de visualiser la **quantit√© de mobilisations** selon les quartiers de Londres, **depuis 2009**,\
              en mettant √©galement en avant le type d'incident. Les quartiers sont tri√©s de fa√ßon descendante selon la moyenne du Temps de R√©ponse ")
  path_to_plot3 = "plot3.html" 
  with open(path_to_plot3,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=1000, height=450)

  with st.expander(label = "Lecture du graphique"):
    st.write("**Constat :** Nous observons que certains des quartiers qui ont un bon temps de r√©ponse ont aussi une grosse proportion de fausses alarmes. Aussi,\
             la quantit√© d'incidents ne semble pas influencer la performance de rapidit√© du temps de r√©ponse. Le type d'incident ne doit donc que partiellement\
            influencer la disparit√© des performances par quartier.")


#### PAGE 4 : MODELISATION

if page == pages[3]:
  st.header("Modelisation")

  #df = pd.read_csv("df_final2.csv", low_memory = False, index_col = 0)

  st.write("En premier lieu nous devons nettoyer notre jeu de donn√©es et supprimer les colonnes que nous avions gard√© pour la DataVisualisation.\
  \nNous supprimons les colonnes d'identification (ID), la colonne DateOfCall qui r√©p√®tent les informations de Day0fCall, MonthOfCall et CalYear ainsi que les colonnes TurnoutTimeSeconds et TravelTimeSeconds afin d‚Äô√©viter une fuite donn√©e qui donneraient √† tort une surperformance √† notre mod√®le de pr√©diction, car elles permettent de calculer notre variable cible.")

  st.write("Ensuite nous r√©duisons notre jeu de donn√©es en ne gardant que les informations post√©rieures √† 2015 car une dizaine de caserne ont ferm√© fin 2014 dans le cadre d‚Äôun plan de secours de sauvegarde financi√®re. \
  \nCe choix a √©t√© fait afin de ne pas fausser les estimations sur ces casernes qui seraient sous repr√©sent√©es et afin √©galement de supprimer les Nans pr√©sents dans la colonne Distance pour ces casernes.")

  st.write("Pour continuer ce nettoyage nous avons modifi√© le type de certaines variable num√©rique en cat√©gorielle car ce sont des indicateurs temporels et qu‚Äôil ne faut pas que le mod√®le de pr√©diction cherche √† les quantifier ou les ordonner.")


  # REDUCTION DE DIMENSIONS
  st.subheader("Reduction de dimensions")

  st.write("Nous avons donc maintenant un jeu de donn√©es de taille `(1295782, 20)`. Nous avons donc d√©cid√© de tenter une r√©duction de dimensions, apr√®s avoir encod√© nos variables cat√©gorielles et standardiser nos donn√©es, nous nous retrouvons avec 351 colonnes et une explication de la variance comme suit : ")
  
  st.image("pca_variance.png")
  with st.expander(label = "Lecture du graphique"):
    st.write("Nous  constatons une chute √† environ 40 nombres de facteurs, voyons voir ce que cela repr√©sente en terme de pourcentage.")

  st.image("pca_ratio.png")
  with st.expander(label = "Lecture du graphique"):
    st.write("Cela ne repr√©sente que 30% de notre jeu de donn√©es, c'est peu. Par curiosit√©, nous visualisons avec deux axes ce que cela repr√©sente.")

  st.image("cercle.png")
  with st.expander(label = "Lecture du graphique"):
    st.write("Comme nous nous y attendions, cela n'est pas tr√®s parlant, plusieurs groupes de variables semblent √™tre bien corr√©l√©es entre elles mais il n'y a aucun int√©r√™t √† r√©duire ici les dimensions sur deux axes puisque les corr√©lations avec les axes sont tr√®s faibles, la plupart d‚Äôentre elles ne d√©passent m√™me pas les 0.2, -0.2. \n\n Nous ne pouvons donc pas nous aider des r√©ductions de dimensions pour r√©duire notre jeu de donn√©es.")
    
  # CORRELATIONS
  st.subheader("Corr√©lations")

  st.write("Afin d‚Äôoptimiser notre mod√®le de pr√©dictions nous d√©cidons d‚Äô√©tudier les corr√©lations entre les variables explicatives et la variable cible.")

  st.markdown("- ##### Variables Num√©riques")

  # Heatmap
  st.image("heatmap.png")
  with st.expander(label = "Lecture du graphique"):
    st.write("A l'aide de cette heatmap, nous d√©cidons de supprimer les colonnes CallCount et MobilisationTime car ce sont celles qui sont le moins corr√©l√©es √† notre variable cible AttendanceTimeSeconds.")
  

  st.markdown("- ##### Variables Cat√©gorielles")
  # Test Anova
  anova = pd.read_csv("anova.csv")
  st.dataframe(anova)

  with st.expander(label = "Lecture du tableau"):
    st.write("Le r√©sultat est sans appel, toutes nos variables explicatives sont corr√©l√©es √† notre variable cible. Nous d√©cidons donc de toutes les garder.")



  # MODELISATION
  st.subheader("Mod√©lisation")  

  st.write("Notre objectif est d‚Äôobtenir un score de test sup√©rieur √† 70%.\
  \nNotre variable cible est une variable continue, ce qui signifie que nous avons √† faire √† une m√©thode de r√©gression. Apr√®s encodage de notre jeu de donn√©es\
   nous avons un data frame de taille `(1295782 , 363)`.\
  \n\n Notre t√¢che sera d‚Äôestimer en fonction des informations fournies, le temps d‚Äôintervention potentiel.\
  \n \nNous allons essayer les 3 algorithmes suivants :") 
  st.markdown("- Linear Regression \
  \n- Decision Tree Regressor \
  \n- Random Forest Regressor")

  st.write("Pour l‚Äô√©valuation de la performance de nos pr√©dictions nous disposons du MSE, RMSE, MAE et score R2.")

  #####  RESULTAT DES 3 MODELES REGRESSION##### 
 
  tab1, tab2, tab3 = st.tabs(["Linear Regression", "Decision Tree Regressor", "Random Forest Regressor"])

  with tab1:
    st.image("linear_reg.png")
   
  with tab2:
    st.image("dtr.png")
   
  with tab3:
    st.image("rf.png")

  st.write("Au vu des r√©sultats du score r2, la m√©trique la plus lisible, nous avons pu d√©terminer que le Random Forest est l‚Äôalgorithme le plus performant.\
  \nCependant, nous avons un grand dataframe et ce mod√®le est tr√®s √©nergivore (Plus d‚Äôune heure d'execution).\
  \nNous d√©cidons donc de convertir nos valeurs dans notre variable cible en diff√©rentes classes afin de simplifier notre pr√©dictions en une classification.")

  st.write("Pour d√©terminer la s√©paration de ces classes, nous observons la distribution de notre variable cible.")

  path_to_plot6bis = "plot6_bis.html" 
  with open(path_to_plot6bis,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=800, height=400)

  st.write("Nous d√©cidons de s√©parer notre variables en 5 classes avec , 1 classe par quartile puis 1 classe pour les valeurs extr√™mes.")

  path_to_plot7bis = "plot7_bis.html" 
  with open(path_to_plot7bis,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=910, height=450)


  st.write("Nous allons relancer 3 nouveaux mod√®les , de classification cette fois ci :") 
  st.markdown("- Logistic Regression \
  \n- Decision Tree Classifier \
  \n- Random Forest Classifier")

  st.write("Cette fois-ci, √©tant donn√© qu‚Äôil s‚Äôagit d‚Äôun mod√®le de classification, nous utiliserons le R2 score, le rapport de classification ainsi qu‚Äôun tableau de confusion pour √©valuer la performance des diff√©rents mod√®les. L‚Äôensemble de ces m√©triques nous apporte des informations importantes sur notre r√©sultat.")


  #####  RESULTAT DES 3 MODELES CLASSIFICATION (5 CLASSES) ##### 
  tab1, tab2, tab3 = st.tabs(["Logistic Regression", "Decision Tree Classifier", "Random Forest Classifier"])

  with tab1:
    st.image("lr5.png")
    
  with tab2:
    st.image("dt5.png")

  with tab3:
    st.image("rf5.png")
    

  st.write("Random Forest est encore une fois le mod√®le le plus performant et cette fois-ci l'ex√©cution est nettement plus rapide(moins de 10min), nous allons approfondir nos recherches sur ce mod√®le.\
  \n\nTout d'abord nous allons modifier la classification car beaucoup de valeurs √©cart√©es se retrouvent dans la m√™me classe,nous allons donc les r√©partir par temps similaire plut√¥t que par part √©gales de valeurs. En effet la classe 5 a un tr√®s mauvais recall, cela se comprend par le fait qu‚Äôil s‚Äôagit de valeurs extr√™mes, elles sont moins bien repr√©sent√©es pour commencer mais aussi elles sont moins logiques pour le mod√®le de pr√©dictions donc plus difficile encore √† pr√©dire.\
  \n\nEnsuite nous √©tudierons les features importantes afin de visualiser si certaines colonnes peuvent √™tre supprim√©es.\
  \n\nPour finir nous nous pencherons sur les hyperparam√®tres.")

  # ETAPE 1: LES CLASSES
  st.markdown("- ##### Etape 1: Retravailler les classes")

  st.write("Nous d√©cidons de r√©duire nos classes, afin de d√©terminer la s√©paration de ces classes, nous observons les zones o√π se regroupent les temps les plus similaires. Pour cela nous ferons deux test:\
  \n\n Le premier test avec 4 classes. \
  \nPour ce qui est de la classe 4 nous d√©cidons d‚Äôy inclure les valeurs les plus fortes plus les valeurs extr√™mes ce qui semble √™tre le plus logique pour la compr√©hension de ces valeurs extr√™mes.\
  \nNous avons donc :\
  \nClasse 1(Temps rapide) : De 0 √† 2:48 (2 min et 48 sec)\
  \nClasse 2(Temps moyen): De 2:48 √† 5:48\
  \nClasse 3(Temps long): De 5:48 √† 10:24\
  \nClasse 4(Temps tr√®s long) : De 10:24 √† 20min.")

  path_to_plot8 = "plot8.html" 
  with open(path_to_plot8,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=910, height=450)

  st.write("Le deuxi√®me test avec 3 classes. \
  \nNous avons donc :\
  \nClasse 1(Temps rapide) : De 0 √† 4:04 (2 min et 48 sec)\
  \nClasse 2(Temps moyen): De 04:04 √† 10:24\
  \nClasse 3(Temps long): De 10:24 √† 20min")

  path_to_plot9 = "plot9.html" 
  with open(path_to_plot9,'r') as f:
    html_data = f.read()
  st.components.v1.html(html_data,width=910, height=450)

  st.write(" Observons les r√©sultats")

  

  ##### RESULTAT DES 2 MODELES (4 classes et 3 classes) ##### 

  option = st.selectbox(
    'Choissisez le nombre de classes pour voir le r√©sultat',
    ('4 classes', '3 classes'))
  
  if option == "4 classes":
    st.image("rf_4.png")

  if option == "3 classes":
    st.image("rf_3.png")


  with st.expander(label = "Lecture des r√©sultats"):
    st.write("Les classes sont toutes bien pr√©dites et nous avons un tr√®s bon score ainsi que de bons r√©sultats de precision, recall et donc de F1\
    \nNous allons maintenant tenter d'affiner la performance gr√¢ce aux hyperparam√®tres.")

 
  # ETAPE 2 : HYPERPARAMETRES
  st.markdown("- ##### Etape 2 : Hyperparam√®tres")

  st.write("Afin de conna√Ætre rapidement quelles seraient les meilleures hyperparam√®tres, nous allons utiliser la validation crois√©e (cross-validation) pour √©valuer diff√©rentes combinaisons d'hyper param√®tres et choisir celle qui donne les meilleures performances.\
  \nPour cela, nous nous servirons de SearchGridCV dont voici les r√©sultats: ")

  st.image("SearchGridCV.png")
  st.write("R√©sultat:")
  st.image("Search_results.png")

  st.write("Nous garderons donc la configuration suivante")
  st.image("best_hyperparametre.png")

  st.write("Voici le r√©sultat final")

  ##### DERNIER RESULTAT  #####

  st.image("rf3h.png")


  st.success("Nous avons atteint notre objectif !\
  \n\n Nous avons l√©g√®rement am√©lior√© notre pr√©diction, le r√©sultat est atteint et nous en sommes tr√®s satisfait.",icon ="üéâ")

 # FEATURES IMPORTANCES
  st.markdown("- ##### Features Importances")
  st.write("Nous allons √©tudier les feature importances pour deceler quelles sont les variables ayant le plus de poids et quelles conclusions pouvons nous tirer de notre travail de mod√©lisation")

  st.image("features.png")

  with st.expander(label = "Lecture des r√©sultats"):
   st.write("Etant donn√© que nos variables cat√©gorielles ont √©t√© encod√©, nous avons un affichage de ces variables par valeurs.\
    \n\nNous automatisons un calcul qui nous donnera la feature importance par variable compl√®te.\
  Nous constatons que les variables PropertyCategory, StopCodeDescription, SpecialServiceType et IncidentGroup sont les moins impactantes sur le jeu de donn√©es.") 
   
  st.image("full_features.png")

  st.write(":red[***Notre avis m√©tier***]")
    st.write("La variable ayant le plus grand impact est DelayCode, nous avions pu constater gr√¢ce aux datavisualisation que le motif de retard qui impactait le plus notre variable cible est le fait d‚Äôavoir une adresse incompl√®te. Une recommandation que nous pourrions faire et de lancer une campagne p√©dagogique aupr√®s de l‚Äô√©quipe du centre d‚Äôappel pour mieux capturer et communiquer l'information de l'adresse.
  La seconde variable est Distance, de l√† on peut se demander si l‚Äôattribution des casernes par rapport au lieu des incidents est optimal et dans le cas contraire retravailler la repartition.
  Pour finir, nous avons la variable HourOfCall et nous supposons que les heures de grand trafic ou les heures o√π le volume d‚Äôincident sont les plus importants sont ceux qui font augmenter le temps de r√©ponse mais √† notre niveau nous ne pouvons pas proposer de solution si ce n‚Äôest une augmentation des effectifs, ce qui doit assur√©ment √™tre une probl√©matique d√©j√† connue de la LFB.")





#### PAGE 5 : CONCLUSION  

if page == pages[4]:
  st.markdown(" ")

  left_co, cent_co,last_co = st.columns(3)
  
  with cent_co:
      st.image('lfb1.svg.png', width = 300)

  st.header('Conclusion')

  st.subheader("Notre parcours : R√©ussites et Difficult√©s")

  st.markdown('**:red[Variable cible]**')

  st.markdown("En premier lieu, nous avions d√©cid√© de travailler sur deux variables cibles :\
              \n 1. ***ResponseTime*** qui correspond au temps √©coul√© entre le moment o√π l'appel est re√ßu par le centre d‚Äôappel 999 et le moment o√π la brigade\
               arrive sur les lieux de l‚Äôincident.\
              \n 2. ***MobilisationTime*** qui repr√©sente le temps pass√© sur l‚Äôincident.\
              \n\n Le premier obstacle que nous avons rencontr√© est d√ª au format des variables temporelles. En effet la variable ResponseTime n‚Äôexiste pas,\
               il fallait donc la calculer √† l‚Äôaide de variables d√©j√† pr√©sentes. Des erreurs d√©tect√©es dans le format des dates nous ont amen√© √† changer cette\
               valeur cible pour :red[***AttendanceTimeSeconds***] qui repr√©sente le temps n√©cessaire √† l‚Äô√©quipe d‚Äôintervention pour se pr√©parer et se rendre sur le lieu\
               de l‚Äôincident.")

  st.markdown(" ")
  
  st.markdown('**:red[D√©couverte et premier Data Cleaning]**')

  st.markdown("La premi√®re chose qui nous a interpell√© est le **volume des donn√©es**, nous avions avant toute modification, deux dataframes que nous avons coupl√©\
               pour obtenir un jeu de donn√©es de taille `(2220718, 58)`.\
              \n\n Il nous a fallu en premier lieu tenter de comprendre l‚Äôint√©gralit√© de ces variables. Certaines semblaient √©videntes, d‚Äôautres, malgr√©\
               la pr√©sence des fichiers de Metadata pr√©sentant une courte description des variables, ont n√©cessit√© la sollicitation de Bi Analyst et Data Analyst\
               du site LFB que nous avons contact√© pour s‚Äôassurer de la bonne interpr√©tation des donn√©es.\
              \n\n Une fois cela fait, nous avons pu d√©cider quelles variables devaient √™tre supprim√©es.\
              Cette √©tape fut complexe par la diversit√© de celles-ci et l‚Äôanticipation requise dans l‚Äôanalyse de leur n√©cessit√© pour le calcul de notre variable\
               cible.\
              \nCependant l‚Äô√©valuation de certaines a √©t√© facilit√©e par le fait qu‚Äôelles repr√©sentaient un doublon sous forme de code.\
              \n\n Apr√®s cela, nous avons pass√© une bonne partie sur le cleaning et l‚Äôenrichissement de nos donn√©es. Nous avons agr√©ment√© notre dataset de diverses\
               variables temporelles ainsi que de la variable :red[***Distance***] qui permet d‚Äô√©tablir la distance entre la caserne d‚Äôo√π d√©colle la brigade et le lieu\
               de l‚Äôincident.")
  
  st.markdown(" ")

  st.markdown('**:red[DataViz]**')

  st.markdown("Nous avons pris plaisir √† cette √©tape de notre projet et avons propos√© un large panel de visualisations.\
              \n\nNous avons cependant √©t√© restreint par les performances r√©duites et les bugs li√©s √† la volum√©trie importante de nos donn√©es\
               et l‚Äôutilisation de certaines librairies.\
              \n\n En effet nous avons fortement appr√©ci√© les visualisations et l‚Äôinterface propos√©es par Plotly express, mais il a √©t√© difficile\
               de les g√©n√©rer pour certains d‚Äôentre eux.\
              \n\n Nous sommes fiers d‚Äôavoir pu proposer des cartes pour nos visualisations que nous avons cr√©√©es gr√¢ce aux fichiers GeoJson que nous nous\
               sommes procur√©s sur le site du London Data Store et √©galement par le biais d‚Äôune des Data Analyst que nous avons contact√©.")

  st.markdown(" ")

  st.markdown('**:red[Mod√©lisation]**')

  st.markdown("Encore une fois lors de cette √©tape, nous avions un gros challenge concernant l‚Äôimportante volum√©trie de nos donn√©es.\
              \n\n Nous avons tent√© de r√©duire au mieux les donn√©es sans perdre d‚Äôinformations pr√©cieuses √† notre travail de pr√©diction. Nous avons pour\
               cela mis en application diverses techniques dont la r√©duction de dimensions, la heatmap, les tests de corr√©lation et d‚Äôind√©pendance ainsi\
               que les features importances. Nous avons √©galement r√©duit nos donn√©es aux dates sup√©rieures √† 2015, puisque fin 2014, 10 casernes ont ferm√©\
               dans le cadre d‚Äôun plan de sauvegarde financi√®re.\
              \n\n Cette abondance de donn√©es nous a d‚Äôailleurs oblig√© √† basculer sur un mod√®le de classification pour am√©liorer la rapidit√© et la pr√©cision\
               de notre mod√®le le plus performant.\
              \n\n Malgr√© notre motivation et par faute de temps, nous avons d√©cid√© de nous concentrer uniquement sur la premi√®re variable cible AttendanceTimeSeconds.\
              Nous avons utilis√© plusieurs mod√®les et l‚Äôavons am√©lior√© jusqu‚Äô√† atteindre notre objectif. Notre meilleur mod√®le retenu est le Random Forest Classifier,\
               avec notre variable cible d√©finie en 3 classes distinctes. Via ce mod√®le, nous avons obtenu un :red[**f1-score de 0.68, 0.7, 0.89 ainsi qu‚Äôun R2 de 78,18**],\
              atteignant donc notre objectif.")
  
  st.markdown(" ")

  st.markdown('**:red[Aller plus loin]**')

  st.markdown("Afin d‚Äôaller plus loin, nous aurions pu, comme nous l'avions fait pour le calcul de la distance, enrichir notre dataset en allant chercher\
               des informations externes pouvant influer sur notre valeur cible, tel que la m√©t√©o, le trafic ou encore la pand√©mie.\
              \n\n Dans le prolongement de notre projet, nous aurions pu aussi utiliser la variable cible *MobilisationTime* pour la mod√©lisation.\
              Avec plus de temps et d'expertise et afin d‚Äôobtenir une meilleure performance, nous aurions pu tester d'autres mod√®les comme les r√©seaux de neurones,\
              SVM‚Ä¶")
  
  st.markdown(" ")

  st.markdown('**:red[Gestion du temps]**')

  st.image("gantt.jpeg")

  st.subheader("Notre retour d'exp√©rience")

  st.markdown("Il nous semblait √©galement important de partager notre retour d‚Äôexp√©rience global sur ce projet, r√©alis√© dans le cadre de notre formation Data Analyst chez DataScientest.\
               \n\n Ce projet fut une belle exp√©rience, nous avons pu mettre en pratique √©norm√©ment d‚Äôoutils que nous avons appris pendant les cours. Nous avons\
               m√™me pu nous aider du projet pour pratiquer plus en profondeur nos apprentissages.\
              \n\n D‚Äôune part, nous avons eu une tr√®s bonne dynamique de groupe, avec une r√©partition des t√¢ches √©quitable, dans un environnement d'entraide\
               et de joie de partager ses connaissances.\
              \n\n D‚Äôautre part, nous tenions √† remercier l‚Äô√©quipe de DataScientest, organisme gr√¢ce auquel nous avons suivi notre formation de Data Analyst,\
               de nous avoir permis d'apprendre dans les meilleures conditions.\
              \n\n Merci √©galement √† notre mentor Mr Yazid Msaadi pour son accompagnement et ses pr√©cieuses recommandations qui nous ont √©t√© d‚Äôune grande aide.\
               Il a su nous guider dans l‚Äôatteinte de notre objectif sur ce projet fil rouge.\
              \n\n Merci √† l‚Äô√©quipe d‚Äôanimation des Masterclass, avec qui nous avons envie d‚Äôen apprendre encore plus et qui nous a permis d‚Äôavoir en temps\
               et en heure les connaissances pour l‚Äôavancement de notre projet. Enfin un merci √† l‚Äô√©quipe de support, proactive sur nos demandes.")

  st.markdown(" ")

  st.subheader("Bibliographie")

  col1, col2 = st.columns(2)

  with col1:
     st.markdown("**Jeux de donn√©es:**\
                 \n - Incident Report\
                 \n - Mobilisation\
                 \n\n **Compr√©hension des variables et enjeux :**\
                 \n - Fire statistics \
                 \n - Fires in Greater London\
                 \n - Incident response time\
                 \n\n **Carte :**\
                 \n - Statistical GIS Boundary Files for London \
                 \n - Borough of London GeoJSON file")
     
  with col2:
     st.markdown("**V√©rification d‚Äôoutlier :**\
                 \n - Incendie de la tour Grenfell \
                 \n - Incendie du 12 Ao√ªt 2012\
                 \n\n **Latitude et longitudes des casernes:**\
                 \n - Majorit√© des casernes \
                 \n - Caserne Dartford\
                 \n - Caserne Esher\
                 \n - Caserne  Hertfordshire")
